{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Travis Lyric Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, re, random\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, LSTM, GRU, Dense, Embedding, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from scipy import spatial\n",
    "import eng_to_ipa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAVIS_PATH = \"TravisScott\"\n",
    "PRINT_SAMPLES = True\n",
    "N_SAMPLES_TO_PRINT = 10\n",
    "EOS_TAG = '<EOL>\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Lyrics\n",
    "Data is imported line by line from the entire discography of Travis Scott gathered from Genius\n",
    "\n",
    "Most notable changes to the lyrics during import are:\n",
    "- removal of adlibs, which is represented in parenthesis in each lyric file\n",
    "- converting all words to lowercase\n",
    "- appending of an end of line tag, \\<EOL>, denoting the end of a line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "\n",
    "song_counter = 0\n",
    "if os.path.exists(TRAVIS_PATH):\n",
    "    for file_name in os.listdir(TRAVIS_PATH):\n",
    "        file_path = os.path.join(TRAVIS_PATH, file_name)\n",
    "        if os.path.isfile(file_path) and os.path.splitext(file_path)[1] == \".txt\":\n",
    "            song_counter += 1\n",
    "            for line in open(file_path):\n",
    "                line = line.rstrip()\n",
    "                line = line.replace(',','')\n",
    "                line = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", line)\n",
    "                line = line.lower()\n",
    "\n",
    "                if len(line) > 0:\n",
    "                    line = line.strip()\n",
    "                    line = line + \" \" + EOS_TAG\n",
    "                    lines.append(line.strip())\n",
    "\n",
    "if PRINT_SAMPLES:\n",
    "    for x in random.sample(range(0, len(lines)), N_SAMPLES_TO_PRINT):\n",
    "        print(lines[x].rstrip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_split = [line.split(' ') for line in lines]\n",
    "\n",
    "\n",
    "docs = []\n",
    "for i in range(len(lines_split)):\n",
    "    docs.append(TaggedDocument(lines_split[i], str(i)))\n",
    "\n",
    "\n",
    "d2v = Doc2Vec(docs, vector_size=100, window=3, min_count=1, workers=4, epochs=50)\n",
    "d2v.build_vocab(docs, progress_per=10)\n",
    "\n",
    "word_vector = d2v.wv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_lines = []\n",
    "for line in lines_split:\n",
    "    # Encode line\n",
    "    encoded_line = []\n",
    "    for word in line:\n",
    "        encoded_line.append(word_vector.key_to_index[word] + 1) # Plus 1 to make padding 0\n",
    "    \n",
    "    # Make n-gram sequences of encoded line\n",
    "    for i in range(1, len(encoded_line)):\n",
    "        n_gram_sequence = encoded_line[:i+1]\n",
    "        encoded_lines.append(n_gram_sequence)\n",
    "\n",
    "# Pad sequences (and convert to numpy array)\n",
    "encoded_lines = pad_sequences(encoded_lines)\n",
    "\n",
    "print(\"Shape: {}\".format(encoded_lines.shape))\n",
    "if PRINT_SAMPLES:\n",
    "    for x in random.sample(range(0, len(lines)), N_SAMPLES_TO_PRINT):\n",
    "        print(encoded_lines[x])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"NUMBER OF SONGS\", song_counter)\n",
    "print(\"NUMBER OF LINES\", len(lines))\n",
    "print(\"NUMBER OF WORDS IN DICT\", len(word_vector.index_to_key))\n",
    "print(\"MAXIMUM LENGTH OF A LINE\", encoded_lines.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train data, X, and train labels Y\n",
    "X = encoded_lines[:,:-1]\n",
    "labels = encoded_lines[:,-1]\n",
    "Y = to_categorical(labels, num_classes=len(word_vector.key_to_index) + 1)\n",
    "\n",
    "print(\"X shape: {}, Y shape: {}\".format(X.shape, Y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "We train a model to predict the next word based on previous words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(Y.shape[1], 256, input_length=X.shape[1]))\n",
    "model.add(Bidirectional(LSTM(128)))\n",
    "model.add(Dense(Y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Use when loading weight\n",
    "#model.load_weights(\"guttaNN/guttaNN\")\n",
    "\n",
    "# Use when fitting model\n",
    "history = model.fit(X, Y, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation of text\n",
    "Given a trained final model, we can now generate some text with the following procedure:\n",
    "1. Predict the top 3 next words for every word in the dictionary\n",
    "2. For each of these 3 words together with the prior word, iteratively predict the next words until an \\<EOS> tag appears\n",
    "3. Convert each encoded sentence back to its original form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_lines = []\n",
    "for word in range(len(word_vector.key_to_index)):\n",
    "    if word in [0,1]:   # Padding or <EOL> tag\n",
    "        continue\n",
    "    ohe_word = pad_sequences([[word]], maxlen=X.shape[1])\n",
    "    prediction = model(ohe_word)\n",
    "    top_3 = np.argpartition(prediction, -3)[0][-3:]\n",
    "    for succ in top_3:\n",
    "        succ_sentence = [word, succ]\n",
    "        for _ in range(len(ohe_word[0])):  # Max of prediction is max length of input array\n",
    "            ohe_word_succ = pad_sequences([succ_sentence], maxlen=X.shape[1])\n",
    "            succ_prediction = model(ohe_word_succ)\n",
    "            succ_next_word = np.argmax(succ_prediction)\n",
    "            succ_sentence.append(succ_next_word)\n",
    "            if succ_next_word in [0, 1]: # Padding or <EOL> tag\n",
    "                break\n",
    "        #print(succ_sentence)\n",
    "        generated_sentence = [word_vector.index_to_key[i - 1] for i in succ_sentence]\n",
    "\n",
    "        generated_lines.append(generated_sentence)\n",
    "\n",
    "if PRINT_SAMPLES:\n",
    "    for i in range(N_SAMPLES_TO_PRINT):\n",
    "        print(random.choice(generated_lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation of a verse\n",
    "Now that we have a database of lines, we can generate a verse by combining these line.\n",
    "For a random start line we calculate the cosine similarity between the start line and all of the other lines in the database. This is for creating verses with context.\n",
    "Then, we look at how each of these lines rhyme with each other, as this is a key aspect for generating rap lyrics. \n",
    "Rap verses typically has lines of sizes close to each other. For instance, it would be weird if one lines have a length of 5 words, and the other line has a length of 20. Therefore we remove all sentences longer or shorter than a threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top similar lines\n",
    "generated_lines_vector = [d2v.infer_vector(line) for line in generated_lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k_similar_lines(line, k=100):\n",
    "    line_vector = d2v.infer_vector(line)\n",
    "    cosine_sim = [spatial.distance.cosine(line_vector, lv) for lv in generated_lines_vector]\n",
    "    top_k_index = np.argpartition(cosine_sim, -k)[-k:]\n",
    "    return [generated_lines[index] for index in top_k_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_line_length(line, line_to_compare):\n",
    "    return 1 - (abs(len(line)-len(line_to_compare)) / max(len(line), len(line_to_compare)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_end_rhymes(line):\n",
    "    end_rhymes = []\n",
    "    for word in line:\n",
    "        word_rhymes = []\n",
    "        ipa_word = eng_to_ipa.convert(word, retrieve_all=True, keep_punct=False)\n",
    "        for ipa in ipa_word:\n",
    "            ipa_rhymes = \"\"\n",
    "            for i in ipa:\n",
    "                if i not in 'bcdfghjklmnpqrstvwxz':\n",
    "                    ipa_rhymes += i\n",
    "            word_rhymes.append(ipa_rhymes)\n",
    "        end_rhymes.append(word_rhymes)\n",
    "    return end_rhymes\n",
    "\n",
    "def calculate_end_rhyme(line, line_to_compare):\n",
    "    line_ipa = get_end_rhymes(line)\n",
    "    line_to_compare_ipa = get_end_rhymes(line_to_compare)\n",
    "\n",
    "    rhyme_count = 0\n",
    "    max_length = min(len(line_ipa), len(line_to_compare_ipa))\n",
    "    for l in range(max_length):\n",
    "        match = False\n",
    "        for x in line_to_compare_ipa[len(line_to_compare_ipa) - l - 1]:\n",
    "            if x in line_ipa[len(line_ipa) - l - 1]:\n",
    "                match = True\n",
    "        if match:\n",
    "            rhyme_count += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return rhyme_count\n",
    "\n",
    "def get_rhyme_candidates(line, similar_lines):\n",
    "    # Remove <EOL>\n",
    "    line = line[:-1]\n",
    "    rhyme_scores = []\n",
    "    for x in similar_lines:\n",
    "        x = x[:-1]\n",
    "        rhyme_scores.append(calculate_end_rhyme(line, x))\n",
    "    return sorted(range(len(rhyme_scores)), key=lambda k: rhyme_scores[k], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LENGTH_THRESHOLD = 0.75\n",
    "\n",
    "for j in range(10):\n",
    "    # Choose a random starting sentence\n",
    "    generated_verse = [random.choice(generated_lines)]\n",
    "    while(len(generated_verse) < 8):    # Make a verse of 8 lines\n",
    "        cosine_lines = get_top_k_similar_lines(generated_verse[-1], 200)\n",
    "        next_line = None\n",
    "        if len(generated_verse) == 4:\n",
    "            i = 0\n",
    "            next_line = cosine_lines[i]\n",
    "            while next_line in generated_verse or calculate_line_length(generated_verse[-1], next_line) < LENGTH_THRESHOLD:\n",
    "                i += 1\n",
    "                next_line = cosine_lines[i]\n",
    "        else:\n",
    "            rhyme_index = get_rhyme_candidates(generated_verse[-1], cosine_lines)\n",
    "            i = 0\n",
    "            next_line = cosine_lines[rhyme_index[i]]\n",
    "            while next_line in generated_verse or next_line[-2] == generated_verse[-1][-2] or calculate_line_length(generated_verse[-1], next_line) < LENGTH_THRESHOLD:\n",
    "                i += 1\n",
    "                next_line = cosine_lines[rhyme_index[i]]\n",
    "        generated_verse.append(next_line)\n",
    "    print(\"-----  VERSE {}\".format(j+1))\n",
    "    for line in generated_verse:\n",
    "        print(line)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
